from utils import llm_call

def loop_workflow(user_query: str, evaluator_prompt: str, max_retries: int = 5) -> str:
    """
    Reâ€‘run the summarisation + evaluation loop until the evaluator says PASS
    or until max_retries attempts have been made. Returns the last summary.
    """
    retries = 0
    while retries < max_retries:
        print(f"\n========== ğŸ“ SUMMARY PROMPT (attempt {retries + 1}/{max_retries}) ==========\n")
        print(user_query)

        summary = llm_call(user_query, model="gpt-4o-mini")
        print(f"\n========== ğŸ“ SUMMARY RESULT (attempt {retries + 1}/{max_retries}) ==========\n")
        print(summary)

        final_evaluator_prompt = evaluator_prompt + summary
        evaluation_result = llm_call(final_evaluator_prompt, model="gpt-4o").strip()

        print(f"\n========== ğŸ” EVALUATION PROMPT (attempt {retries + 1}/{max_retries}) ==========\n")
        print(final_evaluator_prompt)

        print(f"\n========== ğŸ” EVALUATION RESULT (attempt {retries + 1}/{max_retries}) ==========\n")
        print(evaluation_result)

        if "Evaluation Result = PASS" in evaluation_result:
            print("\nâœ… Passed! Final summary approved.\n")
            return summary

        retries += 1
        print(f"\nğŸ”„ Retry requiredâ€¦ ({retries}/{max_retries})\n")

        if retries >= max_retries:
            print("âŒ Reached maximum retries. Returning the last summary.")
            return summary

        # Append history for the next attempt
        user_query += f"\nAttemptâ€¯{retries} summary:\n{summary}\n"
        user_query += f"Attemptâ€¯{retries} feedback:\n{evaluation_result}\n\n"


def main() -> None:
    # Article link: https://zdnet.co.kr/view/?no=20250213091248
    input_article = """
OpenAI will release a new model called â€œGPTâ€‘4.5â€ within the next few weeks,
consolidating its previously fragmented generativeâ€‘AI lineup. The company
plans to retire the inferenceâ€‘focused â€œoâ€ series and fold everything back
into the nonâ€‘inference â€œGPTâ€ line.

According to industry sources on Februaryâ€¯13, OpenAI CEO Sam Altman stated
on X (formerly Twitter) the previous day that GPTâ€‘4.5 would launch soon.
Internally codenamed â€œOrion,â€ it will be the last nonâ€‘inference model,
succeeding the current generation â€œGPTâ€‘4o.â€

At present, OpenAI customersâ€”including ChatGPT usersâ€”choose between models
such as â€œGPTâ€‘4o,â€ â€œo1,â€ â€œo3â€‘mini,â€ and â€œGPTâ€‘4.â€ The newest release is
â€œGPTâ€‘4o,â€ an enhanced version of GPTâ€‘4; GPTâ€‘4 debuted in lateâ€¯2023, while
GPTâ€‘4o arrived in earlyâ€¯2024.

OpenAI had planned to unveil GPTâ€‘5 last year, but the launch was postponed
after internal tests showed lowerâ€‘thanâ€‘expected performance. In the interim,
the company promoted the â€œoâ€â€‘series inference models, which boost
performance by allowing longer compute times.

Altman added, â€œStarting with GPTâ€‘5 weâ€™ll merge the inference â€˜oâ€™ series with
the â€˜GPTâ€™ series. We know our model lineup has become complicated, and we
want things to â€˜just workâ€™ rather than make users pick between models.â€
    """

    user_query = f"""
Your task is to summarise the following news article.
If earlier attempts and feedback exist, incorporate that feedback and
produce an improved summary.

Article:
{input_article}
    """

    evaluator_prompt = """
Evaluate the following summary.

## Criteria
1. **Coverage of key content**
   - Must retain main ideas and logical flow of the original.
   - Omitting >15â€¯% of critical information = FAIL.

2. **Accuracy**
   - No distortion of meaning; names, numbers, and dates must be correct.
   - Major misinterpretations or factual errors = FAIL.

3. **Conciseness & readability**
   - Avoid long, repetitive sentences.
   - Natural, clear wording is fine; extremely awkward prose = FAIL.

4. **Grammar & mechanics**
   - >5 spelling/spacing errors = FAIL.
   - Errors that change meaning = FAIL.

## Evaluationâ€‘result format
- If all criteria are met, output exactly **â€œEvaluation Result = PASSâ€**.
- Otherwise list specific issues and suggestions, and output
  **â€œEvaluation Result = FAILâ€** at the end.

Summary to evaluate:
    """

    final_summary = loop_workflow(user_query, evaluator_prompt, max_retries=5)
    print("\nâœ… FINAL SUMMARY:\n", final_summary)


if __name__ == "__main__":
    main()
